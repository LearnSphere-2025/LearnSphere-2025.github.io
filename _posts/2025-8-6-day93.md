---
layout: single
title: "🧠 Daily Conversation Summarization Model Competition"
---

# Kernel Academy AI Bootcamp (August 6, 2025)

I participated in a natural language processing competition to develop a model that summarizes everyday conversations. Through this project, I gained hands-on experience in model experimentation and performance improvement.

---

## 🎯 Project Goal

- **Topic**: Developing an NLP model to summarize daily conversation data
- **Data**: Conversational text → Summary pairs for training/evaluation
- **Model**: Fine-tuned pre-trained language models for summarization

---

## ⚙️ Model and Experiment Strategy

### ✅ Fine-Tuning KoBART

The base model was **KoBART**.\
We fine-tuned it and used **WandB** to search for the best hyperparameters.

📈 **ROUGE Scores (Our Model / Leaderboard Top Score)**

- ROUGE-1: **0.5668 / 0.5427**
- ROUGE-2: **0.3781 / 0.3437**
- ROUGE-L: **0.4795 / 0.4521**

We achieved performance good enough for real-world applications, and our team ranked **2nd on the final leaderboard**.

---

### 🔍 Exploring Various Models

To further boost performance, we explored various Korean-specific models available on **Hugging Face**.

- We experimented with the **Korean T5 (KoT5)** model, but due to **GPU memory limitations**, we couldn't complete training and saw no improvement in performance.

---

## 🛠 Data Augmentation and Ensembling

### 🔁 Data Augmentation

- We tried a simple augmentation by doubling the training data size, but due to the nature of the summarization task, **the effect was minimal**.

### 🤝 Model Ensembling

- Combining predictions from multiple models through **ensemble techniques** did help **improve performance**.

> ⚠️ However, unlike computer vision tasks, we encountered frequent **errors during code generation and execution**.\
> The **vibe coding** style (ad hoc debugging and coding) consumed a lot of time.

---

## 🤯 Challenges Faced

- Too many variables and conditions made it **difficult to design clear experiments**
- **Server environment and memory limitations** restricted model experimentation
- **Time-consuming debugging** and difficulty in reproducing results

---

## 📌 Reflections & Lessons Learned

- In NLP, **choosing the right model and hyperparameter tuning** is critical
- Compared to simple augmentation, **preprocessing, ensembling, and tuning** were more effective
- In complex experiments, having a **clear experimental plan** is essential

---

#### 🔖 Hashtags

`#NLP #KoBART #Huggingface #ConversationSummarization #AIBootcamp #UpstageAILab #FastCampus #NLProject #WandB #ModelEnsemble`
