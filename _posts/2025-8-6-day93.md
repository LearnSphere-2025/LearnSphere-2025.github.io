---
layout: single
title: "Daily Conversation Summarization Model Competition ðŸ§ "
---

# Kernel Academy AI Bootcamp (August 6, 2025)

I participated in a natural language processing competition to develop a model that summarizes everyday conversations. Through this project, I gained hands-on experience in model experimentation and performance improvement.

---

## ðŸŽ¯ Project Goal

- **Topic**: Developing an NLP model to summarize daily conversation data
- **Data**: Conversational text â†’ Summary pairs for training/evaluation
- **Model**: Fine-tuned pre-trained language models for summarization

---

## âš™ï¸ Model and Experiment Strategy

### âœ… Fine-Tuning KoBART

The base model was **KoBART**.\ I fine-tuned it and used **WandB** to search for the best hyperparameters.

ðŸ“ˆ **ROUGE Scores (Our Model / Leaderboard Top Score)**

- ROUGE-1: **0.5622 / 0.5531**
- ROUGE-2: **0.3582 / 0.3382**
- ROUGE-L: **0.4784 / 0.4626**
- Final-score : **46.6262 / 45.1329**

I achieved performance good enough for real-world applications, and our team ranked **2nd on the final leaderboard**.

---

### ðŸ” Exploring Various Models

To further boost performance, I explored various Korean-specific models available on **Hugging Face**.

- I experimented with the **Korean T5 (KoT5)** model, but due to **GPU memory limitations**, I couldn't complete training and saw no improvement in performance.

---

## ðŸ›  Data Augmentation and Ensembling

### ðŸ” Data Augmentation

- I tried a simple augmentation by doubling the train data size, but due to the nature of the summarization task, **the effect was minimal**.

### ðŸ¤ Model Ensembling

- Combining predictions from multiple models through **ensemble techniques** did help **improve performance**.

> âš ï¸ However, unlike computer vision tasks, I encountered frequent **errors during code generation and execution**.\
> The **vibe coding** style (ad hoc debugging and coding) consumed a lot of time.

---

## ðŸ¤¯ Challenges Faced

- Too many variables and conditions made it **difficult to design clear experiments**
- **Server environment and memory limitations** restricted model experimentation
- **Time-consuming debugging** and difficulty in reproducing results

---

## ðŸ“Œ Reflections & Lessons Learned

- In NLP, **choosing the right model and hyperparameter tuning** is critical
- Compared to simple augmentation, **preprocessing, ensembling, and tuning** were more effective
- In complex experiments, having a **clear experimental plan** is essential

---

#### ðŸ”– Hashtags

`#NLP #KoBART #Huggingface #ConversationSummarization #AIBootcamp #UpstageAILab #FastCampus #NLProject #WandB #ModelEnsemble`
