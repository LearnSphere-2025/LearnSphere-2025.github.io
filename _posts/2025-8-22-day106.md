---  
layout: single
title: "Understanding LangChain: Indexing, Retrieval, and Generation ✨"  
---

# Kernel Academy AI Bootcamp (August 22, 2025)

Today, I’ll summarize what I’ve learned about **LangChain**.  
LangChain is a framework that helps extend the capabilities of Large Language Models (LLMs) by connecting them with external tools, databases, and workflows.  

---

## 🪢 What is LangChain?

LangChain provides several key components:  

- **Model I/O**: Prepare prompts, call language models, and receive results  
- **Retrieval**: Index and search documents from vector stores  
- **Memory**: Remember past interactions (short/long-term)  
- **Callbacks**: Handle various events during execution  
- **Agents**: Function calling and interaction with external systems  
- **Chains**: Easily combine multiple processes into workflows  

In short, it extends beyond the basic *prompt → response* cycle and enables more powerful applications.  

---

## 🔎 Indexing

**Indexing** is the process of transforming data into a searchable format.  
In LangChain, this usually involves:  

1. Loading documents (Documents Loader)  
2. Splitting text into chunks  
3. Embedding the text into vectors using an embedding model  
4. Storing vectors in a vector database (Vector DB)  

👉 This makes the text retrievable based on semantic similarity, not just keywords.  

---

## 📂 Retrieval

**Retrieval** means fetching the most relevant information from stored documents when needed.  
LangChain uses a `Retriever` module to query the vector store.  

- **Similarity Search**: Finds documents closest to the query  
- **MMR (Maximal Marginal Relevance)**: Balances relevance and diversity  
- **Hybrid Search**: Combines keyword-based and vector-based search  

This allows LLMs to use external knowledge that is not part of their training data.  

---

## ✨ Generation

Finally, **Generation** refers to creating new responses using both the retrieved information and the LLM’s reasoning capabilities.  
LangChain enables **RAG (Retrieval-Augmented Generation)** by combining Retrieval + LLM in one chain.  

Example:  
- User asks: “What is LangChain?”  
- The system retrieves documents related to LangChain (Retrieval)  
- The LLM generates a coherent answer using the retrieved context (Generation)  

This ensures more accurate, up-to-date, and context-aware answers.  

---

## 🌱 Summary  

- **Indexing**: Store data in a vectorized, searchable format  
- **Retrieval**: Find the most relevant information from the vector store  
- **Generation**: Use retrieved data to create meaningful answers with LLM  

The real strength of LangChain lies in its **RAG pipeline**, which enables LLMs to act as **knowledge assistants** powered by external data.  

---

#### 🔖 Hashtags  
`#LangChain #AIbootcamp #LLM #RAG #Indexing #Retrieval #Generation #AIframework`  
'#패스트캠퍼스 #패스트캠퍼스AI부트캠프 #업스테이지패스트캠퍼스 #UpstageAILab #국비지원 #패스트캠퍼스업스테이지에이아이랩 #패스트캠퍼스업스테이지부트캠프'
