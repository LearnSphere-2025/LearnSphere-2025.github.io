---
layout: single
title: "The Evolution of Language Models🌱"
---

# Kernel AI Bootcamp (August 14, 2025)

---

## 📚 1. Basics and Evolution of Language Models

- **Language Models (LM)**: Core technology enabling computers to understand and generate human language.
- **Model Architectures**
  - 🔹 Encoder-based (e.g., BERT): Focused on language understanding.
  - 🔹 Decoder-based (e.g., GPT): Specialized in text generation.
- **Traditional NLP Techniques**
  - N-gram, Bag-of-Words, TF-IDF, BM25, etc.
- **Evaluation Metrics**
  - Perplexity (lower = better), BLEU, METEOR, etc.
- **Pretrained Language Models**
  - Word Embedding: Word2Vec, GloVe, FastText
  - Transformer-based: ELMo → GPT, BERT → RoBERTa, ALBERT, DeBERTa, etc.
- **Seq2Seq Models**
  - XLM, BART, MASS, T5 and more
- **Limitations**
  - Bias, lack of generalization, hallucination issues
- **Future Focus**
  - High-quality data curation, human alignment, real-world applicability

---

## 🚀 2. The Rise of LLMs (Large Language Models)

- **LLMs**: Represent a breakthrough moment in AI development
  - Key concepts: Human Alignment, Scaling Laws, In-Context Learning (ICL)
- **Development Components**
  - Infrastructure, backbone models, high-quality datasets, tuning methods
- **Emerging Trends**
  - Quality and diversity of data emphasized
  - Increased use of synthetic datasets
  - Surge in domain-specific LLMs
  - LLM self-evaluation research
  - Ethical alignment, reduction of toxicity
  - Prompt engineering strategies: Chain-of-Thought, Self-Consistency
  - Tools: LangChain, Scikit-LLM, LLMOps, Augmented LLMs
- **Ecosystem Growth**
  - Open LLM Leaderboard, Hugging Face benchmarks gaining traction

---

## 🌍 3. Multilingual, Multimodal, and Cross-Lingual Model 

### 3.1 Multilingual LLMs

- Multilingual PLMs: mBERT, XLM, mBART, mT5, etc.
- Leading LLMs: PaLM, LLaMA, Falcon, RedPajama, GPT-4, etc.
  - Example: Korean accounts for 0.195% of PaLM2's training data
- Multilingual benchmarks: LASER, FLORES, NLLB200, MEGA, etc.

### 3.2 Multimodal LLMs

- Text + Image: CLIP, BLIP, BLIP-2
- Audio/Video + Text: Whisper, VideoCLIP
- GPT-4, Gemini, Flamingo support various multimodal capabilities

### 3.3 Cross-Lingual Transfer Learning

- Techniques: Instruction Tuning, Further Pretraining, Vocab Extension
- Model Transfer Methods: AMM, XPT
- Embedding Alignment: WECHSEL
- Adapter-Based Fine-tuning: LoRA, QLoRA

---

## 🧠 4. OpenAI and the LLM Ecosystem

### 🏢 OpenAI Overview

- Founded in 2015 (Elon Musk, Sam Altman, etc.)
- Partners closely with Microsoft
- Key models: GPT series, Whisper, DALL·E, Codex, etc.
- 2023: Sam Altman’s firing and return gained widespread attention

### 🛠️ OpenAI APIs and Tools

- **Chat Completions API**: Follows system/user/assistant role format
- **Function Calling**: Supports API calls, tools, SQL queries, etc.
- **Embeddings API**: Semantic search, recommendation, similarity
- **Fine-tuning & GPT Builder**: Custom LLM deployment
- **DALL·E & Whisper APIs**: Text-to-image and speech-to-text support

---

## 🧾 Conclusion

The evolution from LM to LLM marks a major leap in NLP technology — in **scale, capability, and versatility**.  
Emerging LLMs are advancing in:

- 🌐 **Multilingual capabilities**
- 🖼 **Multimodal integration**
- 🔁 **Cross-lingual transfer and adaptation**
- 🔐 **Safety, ethical alignment, and reliability**

---

#### 🔖 Hashtags  
#AIbootcamp #LLM #NLP #OpenAI #MultilingualLLM #MultimodalAI #GPT #FastCampus #UpstageAILab

#패스트캠퍼스 #패스트캠퍼스AI부트캠프 #업스테이지패스트캠퍼스 #UpstageAILab #국비지원 #패스트캠퍼스업스테이지에이아이랩 #패스트캠퍼스업스테이지부트캠프

