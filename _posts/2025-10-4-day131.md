---
layout: single 
title: "Boosting Kaggle Score with Qwen + LoRA ğŸ§ "
---

# Kernel Academy AI Bootcamp (Oct 4, 2025)

In this post, I share my experience of achieving a high score in a **Kaggle competition** by applying the **Qwen3-4B + LoRA** model.  
During the LLM competition at the AI bootcamp, I didn't have enough time to try out various experiments â€” so this was a great chance to revisit those ideas.

---

## ğŸ From DeBERTa to Qwen3-4B + LoRA: A Leap in Performance

Initially, I submitted a solution using the **DeBERTa-v3-base** model and achieved a **0.823** score on the leaderboard.  
However, after studying publicly shared notebooks from the Kaggle community that used **Qwen3-4B + LoRA**, I modified and fine-tuned the code to submit a new solution â€” which resulted in a significantly improved score of **0.945**.

![Kaggle Leaderboard](assets/images/kaggle-day2.jpg)

This experience clearly showed me how powerful LLM-based approaches can be compared to traditional pre-trained models.

---

## ğŸ“‚ Learning from Public Kaggle Code

Hereâ€™s how I approached the task:

1. Studied top-scoring usersâ€™ code and pipeline structures  
2. **Copied code and fixed errors / created a new formatting scheme** to test for performance gains  
3. Replaced the original model with an LLM, and applied **LoRA** to optimize memory usage and boost performance  

---

## ğŸŒ A World Full of AI Experts

What I learned through this experience:

- There are so many incredibly skilled people out there in the AI community  
- Even beginners can achieve strong results by effectively leveraging publicly shared code on Kaggle  
- I hope to keep learning and growing â€” and eventually work alongside these experts and contribute to the field

---

#### ğŸ”– Hashtags  
`#AI_Bootcamp #KernelAcademy #KernelAcademyBootcamp #KernelAcademyAI #Kaggle`
