---
layout: single
title: "Kaggle training my third experimentğŸ§ "
---

# Kernel Academy AI Bootcamp (Oct 9, 2025)

During the short but meaningful **Chuseok holiday week (Korean Thanksgiving)**, I participated in an education-themed Kaggle competition and explored **LLM fine-tuning and inference** using open-source models.  
This post reflects on the hands-on learning process and challenges I overcame.

---

## ğŸ”§ DeepSeek-R1-Distill-Qwen-7B with LoRA Fine-tuning & Inference

I found a **pretrained distilled model** â€” `DeepSeek-R1-Distill-Qwen-7B` â€” on **Hugging Face** and applied **LoRA (Low-Rank Adaptation)** fine-tuning on it. 

Through this process, I was able to understand how to:

- Adapt large language models using parameter-efficient fine-tuning
- Balance training time and GPU memory constraints
- Manage inference with limited resources in the Kaggle environment

After fine-tuning, I built an **inference pipeline** within a Kaggle notebook. Iâ€™ll be submitting the final notebook this **Saturday**, once Kaggle GPU access is restored.

![kaggle](/assets/images/kaggle-day7.jpg)

---

## ğŸ¯ Kaggle Experience: Learning by Doing

It was my **first time participating in a Kaggle competition**, and Iâ€™ve grown much more comfortable with the platform:

- Understanding submission formats and scoreboards
- Managing computational resources (e.g., time limits, GPU)
- Reproducibility & sharing via notebooks
- Engaging with community discussion
- Participating even during a short national holiday!

Iâ€™m glad I could apply my skills to a competition related to **education**â€”a field I care deeply about.

---

### ğŸ”– Hashtags  
`#AIë¶€íŠ¸ìº í”„ #ì»¤ë„ì•„ì¹´ë°ë¯¸ #ì»¤ë„ì•„ì¹´ë°ë¯¸ë¶€íŠ¸ìº í”„ #ì»¤ë„ì•„ì¹´ë°ë¯¸AIë¶€íŠ¸ìº í”„ #Kaggle`
