---
layout: single
title: "Kaggle training my third experiment🧠"
---

# Kernel Academy AI Bootcamp (Oct 9, 2025)

During the short but meaningful **Chuseok holiday week (Korean Thanksgiving)**, I participated in an education-themed Kaggle competition and explored **LLM fine-tuning and inference** using open-source models.  
This post reflects on the hands-on learning process and challenges I overcame.

---

## 🔧 DeepSeek-R1-Distill-Qwen-7B with LoRA Fine-tuning & Inference

I found a **pretrained distilled model** — `DeepSeek-R1-Distill-Qwen-7B` — on **Hugging Face** and applied **LoRA (Low-Rank Adaptation)** fine-tuning on it. 

Through this process, I was able to understand how to:

- Adapt large language models using parameter-efficient fine-tuning
- Balance training time and GPU memory constraints
- Manage inference with limited resources in the Kaggle environment

After fine-tuning, I built an **inference pipeline** within a Kaggle notebook. I’ll be submitting the final notebook this **Saturday**, once Kaggle GPU access is restored.

![kaggle](/assets/images/kaggle-day7.jpg)

---

## 🎯 Kaggle Experience: Learning by Doing

It was my **first time participating in a Kaggle competition**, and I’ve grown much more comfortable with the platform:

- Understanding submission formats and scoreboards
- Managing computational resources (e.g., time limits, GPU)
- Reproducibility & sharing via notebooks
- Engaging with community discussion
- Participating even during a short national holiday!

I’m glad I could apply my skills to a competition related to **education**—a field I care deeply about.

---

### 🔖 Hashtags  
`#AI부트캠프 #커널아카데미 #커널아카데미부트캠프 #커널아카데미AI부트캠프 #Kaggle`
