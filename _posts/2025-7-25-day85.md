---
layout: single
title: "BERT 이전의 자연어처리 & NLP 파이프라인🌱"
---

# Kernel Academy AI Bootcamp (July 25, 2025)

이번 포스트는 자연어 처리(NLP)의 흐름을 이해하고, BERT 이전의 주요 모델들과 핵심 개념들을 정리한 내용입니다. 또한, Hugging Face 기반의 NLP 분류 프로젝트 (낚시성 기사 분류)를 구현해보았습니다.

---

## 📌 목차

1. BERT 이전의 주요 NLP 모델 흐름
2. 자연어처리 Task 이해 및 Hugging Face Pipeline
3. NLP 모델 학습 및 추론 실습 (낚시성 기사 분류)

---

## 1. BERT 이전의 자연어 처리 모델 흐름

### 🔁 Seq2Seq (Sequence-to-Sequence)

- 입력 시퀀스를 고정된 벡터로 인코딩한 후 새로운 시퀀스를 생성  
- 응용 분야: **기계 번역**, **질문 생성**, **챗봇**
- 한계: 긴 문장일수록 정보 손실 → **장기 의존성 문제**

### 🔄 RNN과 개선형 구조 (LSTM, GRU)

- 시간 순서를 따라 정보를 전달
- **LSTM**: 게이트 구조로 장기 의존성 완화  
- **GRU**: LSTM보다 간단하지만 유사한 성능

### 🎯 Attention Mechanism

- Encoder의 마지막 벡터 의존 문제 해결
- 디코더가 입력 시퀀스 전체를 참조 → **가중치 기반 정보 선택**
- 장점: 문맥에 맞는 유연한 생성 가능

### ⚡ Transformer (2017)

- "Attention is All You Need"
- **완전한 Attention 기반 모델**
- 병렬 연산 가능 → 대규모 학습에 적합  
- 핵심 구성: Self-Attention, Multi-Head Attention, Positional Encoding, FFN, Residual Connection
- Encoder-Decoder 구조는 유지하지만, 내부적으로 RNN 없이 Attention만 사용

---

## 2. 📘 자연어처리 Task 이해하기

### 🏢 1. Hugging Face와 Pipeline

- **Hugging Face**
    - 2016년 설립된 오픈소스 중심의 NLP 선도 기업
    - 주요 라이브러리: `transformers`, `datasets`, `evaluate` 등
- **Pipeline 모듈 (from `transformers`) : 간단한 추론 목적**
    - pipeline 모듈은 학습없이 빠르게 모델 성능을 테스트할 때 유용합니다. 왜냐하면 `pipeline()`은 내부적으로 다음 단계를 자동으로 처리해 줍니다:
    1. **모델 로딩 (`AutoModelForXXX.from_pretrained()`)**
    2. **토크나이징 (`AutoTokenizer.from_pretrained()`, `tokenizer(text)`)**
    3. **forward 패스 및 추론 (`model(**inputs)`)**
    4. **후처리 및 결과 포맷팅 (argmax, score 변환 등)**
       

### 📊 대표적인 NLP Task 유형

| Task                               | 설명             |
| ---------------------------------- | -------------- |
| **기계 번역 (Translation)**            | 언어 A → 언어 B    |
| **질문 응답 (QA)**                     | 질문에 대한 정답 추출   |
| **정보 추출 (Information Extraction)** | 문장에서 엔티티 추출    |
| **감성 분류 (Sentiment Analysis)**     | 텍스트의 감정 파악     |
| **요약 (Summarization)**             | 긴 문서의 핵심 내용 압축 |


### 🎯 자연어처리 평가지표

| 지표            | 설명                         |
| ------------- | -------------------------- |
| **Precision** | 정답 판정 중 실제 정답 비율           |
| **Recall**    | 실제 정답 중 맞춘 비율              |
| **F1 Score**  | Precision + Recall 조화 평균   |
| **BLEU**      | 생성 텍스트의 유사도 측정 (n-gram 기반) |


**“어떤 지표를 사용할지는 서비스 목적에 따라 달라진다”**

예시:
의료 QA 서비스 → Recall 중요 (놓치면 안 되는 정보)
뉴스 요약 서비스 → ROUGE, BLEU로 내용 보존 여부 확인
고객 피드백 감성 분석 → F1 Score로 균형 평가
스팸 필터링 → Precision 중요 (정상 메시지를 차단하지 않도록)

---

## 3. 📘 자연어처리 Pipeline 이해하기

### 예시 Task: 낚시성 기사 분류 (6개 카테고리)

### 1️⃣ **환경설정**

**목표**: 필요한 라이브러리 설치 + 데이터 불러오기

- **라이브러리**: `transformers`, `datasets`, `sklearn`, `torch`, `pandas` 등
- **데이터**: `train.csv`, `test.csv` (텍스트 + 카테고리 라벨)
- **PyTorch DataLoader 장점**:
    - 배치 학습(Batch Training)
    - 데이터 섞기(Shuffle)
    - 멀티 프로세싱 지원
    - 다른 PyTorch 구성요소들과 높은 호환성

---

### 2️⃣ **데이터셋 구축 + 토크나이징(Tokenizing)**

**목표**: 텍스트 데이터를 모델이 이해할 수 있도록 변환

- **Train/Valid 나누기**: 7.5:2.5로 나눠준 (split) —> 그러나, 보통 8:2 혹은 9:1로 분할
- **토크나이저 불러오기**:
    
    예: `BertTokenizer.from_pretrained('bert-base-uncased')`
    
- **주의**: 모델과 토크나이저는 **같은 checkpoint**여야 함
    
    예: `bert-base-uncased` 모델 ↔ 동일한 `bert-base-uncased` 토크나이저
    
- **출력 형태**: input_ids, attention_mask, label

---

### 3️⃣ **모델 및 토크나이저 로딩**

**목표**: 사전 학습된(pre-trained) 모델 기반 분류기 설정

---
### 4️⃣ **모델 학습: Trainer 사용**

**목표**: 학습을 자동화하고 최적화

- **TrainingArguments 설정**:
    
    로그 저장, 배치 사이즈, 에포크 수, 평가 전략 등 설정
    
- **Trainer 사용 이유**:
    - 학습 루프 간단화
    - 자동 평가, 로깅, 스케줄링, 조기 종료 등 지원
    - `compute_metrics`로 F1/Accuracy/Precision 등 커스터마이징 가능

---
### 5️⃣ **추론(Inference) 및 평가(Evaluation)**

**목표**: 테스트 데이터에 대해 분류 결과 예측하고 평가 지표 확인

- **모델 체크포인트 불러오기**:
    
    ```python
    python
    CopyEdit
    model = AutoModelForSequenceClassification.from_pretrained("results/checkpoint-best")
    ```
    
- **예측 수행**:
    
    ```python
    python
    CopyEdit
    predictions = trainer.predict(test_dataset)
    ```
    
- **평가 지표 예시**: Accuracy, Precision, Recall, F1 Score
- **활용 라이브러리**: `sklearn.metrics`

---
#### 🔖 Hashtags  

#패스트캠퍼스 #패스트캠퍼스AI부트캠프 #업스테이지패스트캠퍼스 #UpstageAILab #국비지원 #패스트캠퍼스업스테이지에이아이랩 #패스트캠퍼스업스테이지부트캠프
