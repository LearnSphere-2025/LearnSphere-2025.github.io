---
layout: single
title: "Kaggle training Qwen 3-4bğŸ§ "
---

# Kernel Academy AI Bootcamp (October 6, 2025)

I performed inference using that saved model and submitted the `submission.csv` to the leaderboard. Hereâ€™s a brief reflection along with a summary of the new full-data training currently in progress.

---

## ğŸ“¦ Fold 1 Model Saved & Inference Performed

Yesterday, I trained the Qwen3-4b model and saved the **weighted model for fold 1**. Today, I used that **single saved model** to run inference and generated a `submission.csv` file. I submitted it to the leaderboard to evaluate the pipeline.

- ğŸ’» Environment: Kaggle Notebook (GPU P100)  
- ğŸ• Inference Time: **About 5 hours**  
- ğŸ“ˆ Score: `0.676`  
  â†’ As expected, the score was relatively low due to using only **1 epoch** and **1 fold**.

![leaderboard](/assets/images/kaggle-day4.jpg)

---

## âš™ï¸ Full Data Training Configuration

Now Iâ€™m training the model using **the entire dataset**, aiming for **maximum performance**. Validation will be done on the training set itself to **monitor overfitting** rather than for tuning.

### âœ… Expected Benefits

- Leverage all samples, especially helpful for **minority class issues**
- Maximize performance by fully utilizing available data

### ğŸ›  Training Settings
```yaml
epochs: 2
batch_size: 4
n_folds: 1
learning_rate: 3.0e-5
weight_decay: 0.01
warmup_ratio: 0.1
gradient_accumulation_steps: 4
gradient_checkpointing: true
early_stopping_patience: 2
```
---

The training is expected to take around **10 hours**, and I'm **hoping for a strong result**.

---

#### ğŸ”– Hashtags  
`#AIë¶€íŠ¸ìº í”„ #ì»¤ë„ì•„ì¹´ë°ë¯¸ #ì»¤ë„ì•„ì¹´ë°ë¯¸ë¶€íŠ¸ìº í”„ #ì»¤ë„ì•„ì¹´ë°ë¯¸AIë¶€íŠ¸ìº í”„ #Kaggle`
