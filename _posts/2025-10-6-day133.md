---
layout: single
title: "Kaggle submitting my first trained Qwen 3-4b🧠"
---

# Kernel Academy AI Bootcamp (October 6, 2025)

I performed inference using that saved model and submitted the `submission.csv` to the leaderboard. Here’s a brief reflection along with a summary of the new full-data training currently in progress.

---

## 📦 Fold 1 Model Saved & Inference Performed

Yesterday, I trained the Qwen3-4b model and saved the **weighted model for fold 1**. Today, I used that **single saved model** to run inference and generated a `submission.csv` file. I submitted it to the leaderboard to evaluate the pipeline.

- 💻 Environment: Kaggle Notebook (GPU P100)  
- 🕐 Inference Time: **About 5 hours**  
- 📈 Score: `0.676`  
  → As expected, the score was relatively low due to using only **1 epoch** and **1 fold**.

![leaderboard](/assets/images/kaggle-day4.jpg)

---

## ⚙️ Full Data Training Configuration

Now I’m training the model using **the entire dataset**, aiming for **maximum performance**. Validation will be done on the training set itself to **monitor overfitting** rather than for tuning.

### ✅ Expected Benefits

- Leverage all samples, especially helpful for **minority class issues**
- Maximize performance by fully utilizing available data

### 🛠 Training Settings
```yaml
epochs: 1
batch_size: 8
n_folds: 1
gradient_accumulation_steps: 2
gradient_checkpointing: true
effective batch size: 16
LoRA trainable params: 3.8M
```
---

The training is expected to take around **10 hours**, and I'm **hoping for a strong result**.

---

#### 🔖 Hashtags  
`#AI부트캠프 #커널아카데미 #커널아카데미부트캠프 #커널아카데미AI부트캠프 #Kaggle`
