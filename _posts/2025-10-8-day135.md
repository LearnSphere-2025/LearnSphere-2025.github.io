---
layout: single
title: "Kaggle training my second experiment🧠"
---

# Kernel AI Bootcamp (Oct 8, 2025)

This is a log of my second fine-tuning experiment with the Qwen3-4B model for a Kaggle competition. Thanks to insights shared by the community, I was able to improve training speed, accuracy, and overall understanding of efficient model usage on Kaggle.

---

## Insights from the Kaggle Community

I learned some key tips from discussions in the Kaggle community forums:

- **Models between 7B and 14B** seem to be the sweet spot for performance and efficiency. Smaller models tend to underperform, while larger ones often exceed Kaggle’s hardware limitations (especially on T4 GPUs).

- **To take full advantage of Kaggle's T4 GPUs, must use `fp16` during inference.**  
I finally understood why my inference was so slow before — I had been skipping half-precision inference.

![discussion](/assets/images/kaggle-day6.jpg)

---

## 🧪 Qwen3-4B + LoRA (Local Training)

Today, I trained the Qwen3-4B model with LoRA on my local machine. Here’s the optimized configuration I used for faster training:

```yaml
experiment: 
  name: qwen3_4b_local_fast
  description: Qwen3-4B fast training (single fold, optimized speed)
  experiment_dir: ./experiments/qwen3_4b_fast

model:
  type: qwen
  name: Qwen/Qwen3-4B
  pretrained_path: ./pretrained/qwen3-4b
  max_length: 256
  device_map: auto
  torch_dtype: bfloat16

lora:
  use_lora: true
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  bias: "none"

training:
  epochs: 2
  batch_size: 8
  eval_batch_size: 16
  learning_rate: 3.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  gradient_accumulation_steps: 2
  gradient_checkpointing: true
  early_stopping_patience: 2
  seed: 42
  fp16: false
  bf16: true

data:
  train_csv: ./data/train.csv
  test_csv: ./data/test.csv
  n_folds: 1
  random_state: 42
```

---
## Validation

After training, I achieved the following validation results — noticeably better than my previous run: eval_accuracy: 0.9147

---

## Inference & Submission Status

I uploaded the weighted inference results to a Kaggle notebook, but haven’t submitted it yet due to GPU limitations. I plan to submit the results once my Kaggle GPU quota resets this Saturday.

---

## Upcoming: DeepSeek-R1-Distill-Qwen-7B + LoRA

My third experiment is already in progress: 👉 DeepSeek-R1-Distill-Qwen-7B + LoRA, known for its strength in math reasoning tasks. Training takes more than 5 hours, so I’ll continue tomorrow and evaluate the results then.

---

### 🔖 Hashtags  
`#AI부트캠프 #커널아카데미 #커널아카데미부트캠프 #커널아카데미AI부트캠프 #Kaggle`
