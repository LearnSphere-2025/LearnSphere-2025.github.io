---
layout: single
title: "AI Research Assistant V5 🎯"
---

# Dynamic AI/ML Paper Search System with LangGraph Workflow

**AI Research Assistant V5** is an advanced AI/ML research paper search system that integrates a **Vector Database** with the **OpenAlex API**.  
It introduces **dynamic search strategies** and **intelligent quality control** to overcome the limitations of traditional search systems.

---

## ✨ Key Innovations in V5

![LangGraph V5.3](Gradio_demo.jpg)
![Gradio_demo](https://github.com/user-attachments/assets/460ce806-bca4-4619-a00f-a8bbda2ddbac)

### 🚀 Dynamic Search Strategy

Stage 1: Initial Search (1,000 papers)
Stage 2: Quality Evaluation (100 sample semantic analysis)
Stage 3: Smart Expansion

Quality >20%: Use 1,000 papers ✓

Quality 10-20%: Expand to 2,500 papers ↗️

Quality <10%: Expand to 5,000 papers ↗️↗️


### 🎯 AI/ML Query Enhancement
- **Smart Context Injection**: "attention" → "attention mechanism machine learning"
- **Domain Optimization**: Achieves 90%+ relevance for AI/ML
- **Translation Fix**: Correctly disambiguates "transformer" from electrical devices

### 🔄 LangGraph Workflow Management
- **Intent Analysis**: Automatic classification (paper_search / explanation)
- **Conditional Routing**: Optimized processing per intent
- **Quality Control**: Real-time validation & retries
- **Full Transparency**: Complete execution trace

### 🛡️ 3-Layer Quality Control
1. **Semantic Filtering**: Threshold ≥0.3
2. **Citation Ranking**: Prioritize highly cited papers
3. **Quality Metrics**: Real-time ratio tracking

---

## 📊 Performance Metrics

### Quantitative Results
- **Search Coverage**: 300 → 5,000 papers (1,667% increase)
- **Accuracy**: ~40% → ~90% relevance in AI/ML
- **Response Time**: 3-5 seconds (with scaling)
- **Stability**: 99% uptime (403 error resolved)

### Test Results

Query: "Find self-attention neural network papers"
✅ 4 high-quality papers (scores: 0.566, 0.530, 0.471, 0.471)
✅ Includes Vaswani et al. Transformer paper

Query: "Search for GPT and large language models"
✅ 4 recent LLM papers (2023–2024)
✅ Includes GPT-4 and ChatGPT research

Query: "Find research on ResNet architecture"
✅ 4 CNN/Deep Learning studies
✅ All directly relevant to ResNet


---

## 🏗️ System Architecture

### Data Structures

Vector Database: 443 papers → 1,429 chunks
OpenAlex API: Dynamic 1K–5K search
Embedding: sentence-transformers/all-MiniLM-L6-v2
Threshold: ≥0.3 for filtering

### Workflow

User Query → LangGraph Engine → Output
↓
[Intent Analysis] → [Vector Search] → [OpenAlex Search] → [Quality Control]
↓ ↓ ↓
443 papers Dynamic 1K–5K Semantic Filter
1,429 chunks Smart Scaling Score ≥0.3



### Main Components and Data

gradio_app_v5.py: Web interface (Gradio)

modules/retriever/advanced_rag.py: Dynamic retriever engine

modules/crawler/openalex_api.py: OpenAlex API client

modules/langchain_helper/search_agent.py: LangGraph workflow

vectorstore/: FAISS Vector DB (443 papers, 1,429 chunks)

Plan_v5.txt: Full project notes & analysis


### Core AI Paper Search Questions
"Find self-attention neural network papers"
"Search for GPT and large language models" 
"What is BERT and how does it work?"
"Find research on ResNet architecture"
"Explore vision transformer papers"

### Sample Result
Title: "Attention Is All You Need"
Authors: Vaswani et al.
Score: 0.566
Citations: 50,000+
Year: 2017

### Tech Stack

Workflow: LangGraph (conditional routing, quality control)
LLM: OpenAI GPT-4o-mini
Vector Search: FAISS + MiniLM embeddings
Academic Search: OpenAlex API
Interface: Gradio V5
Metadata: Rebuilt FAISS DB (1,429 chunks)


