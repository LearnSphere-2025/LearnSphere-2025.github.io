---
layout: single
title: "🧠 Kaggle Competition Day 3 - Fine-tuning Qwen3-4b"
---

# Kernel Academy AI Bootcamp (October 5, 2025)

Today was Day 3 of my Kaggle competition challenge. After discovering that the **Qwen3-4b model** performs well yesterday, I decided to try training it on the competition dataset myself.

---

## 🔍 Model Training Attempt: Fine-tuning Qwen3-4b

Today's goal was to fine-tune the `Qwen3-4b` model on the math misconception dataset. But here’s what actually happened:

- 🖥 **GPU Overload**: Frequent interruptions due to resource limits on Colab  
- 🧩 **Many Code Errors**: Struggled with unfamiliar model structure and bugs  
- ⏳ **Only Completed 1 Fold**: And even that only classified categories of misconceptions — not the misconceptions themselves

Despite spending over 8 hours experimenting, I made limited progress in terms of model performance. Clearly, deeper understanding and more optimization are needed.

![MAP_Fold1](/assets/images/kaggle-day3.jpg)

---

## ⚙️ A Win: Uploading Pretrained Models to Kaggle via CLI

One thing I successfully achieved today:

- ✅ Learned how to **upload pretrained models to Kaggle Notebooks**  
  using **Kaggle API** and **VS Code CLI**

This will be a valuable skill for future experiments and building automated training pipelines.

---

## 📝 Plans for Tomorrow

Tomorrow is Chuseok (Korean Thanksgiving), so I’m not sure how much I’ll be able to study. Still, I plan to revisit today’s code and make improvements:

- Improve data preprocessing
- Consider using parameter-efficient tuning (LoRA / PEFT)
- Adjust batch size and gradient accumulation

---

#### 🔖 Hashtags  
`#AI부트캠프 #커널아카데미 #커널아카데미부트캠프 #커널아카데미AI부트캠프 #Kaggle`


