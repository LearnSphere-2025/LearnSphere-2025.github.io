---
layout: single
title: "ğŸ§  Kaggle Competition Day 3 - Fine-tuning Qwen3-4b"
---

# Kernel Academy AI Bootcamp (October 5, 2025)

Today was Day 3 of my Kaggle competition challenge. After discovering that the **Qwen3-4b model** performs well yesterday, I decided to try training it on the competition dataset myself.

---

## ğŸ” Model Training Attempt: Fine-tuning Qwen3-4b

Today's goal was to fine-tune the `Qwen3-4b` model on the math misconception dataset. But hereâ€™s what actually happened:

- ğŸ–¥ **GPU Overload**: Frequent interruptions due to resource limits on Colab  
- ğŸ§© **Many Code Errors**: Struggled with unfamiliar model structure and bugs  
- â³ **Only Completed 1 Fold**: And even that only classified categories of misconceptions â€” not the misconceptions themselves

Despite spending over 8 hours experimenting, I made limited progress in terms of model performance. Clearly, deeper understanding and more optimization are needed.

![MAP_Fold1](/assets/images/kaggle-day3.jpg)

---

## âš™ï¸ A Win: Uploading Pretrained Models to Kaggle via CLI

One thing I successfully achieved today:

- âœ… Learned how to **upload pretrained models to Kaggle Notebooks**  
  using **Kaggle API** and **VS Code CLI**

This will be a valuable skill for future experiments and building automated training pipelines.

---

## ğŸ“ Plans for Tomorrow

Tomorrow is Chuseok (Korean Thanksgiving), so Iâ€™m not sure how much Iâ€™ll be able to study. Still, I plan to revisit todayâ€™s code and make improvements:

- Improve data preprocessing
- Consider using parameter-efficient tuning (LoRA / PEFT)
- Adjust batch size and gradient accumulation

---

#### ğŸ”– Hashtags  
`#AIë¶€íŠ¸ìº í”„ #ì»¤ë„ì•„ì¹´ë°ë¯¸ #ì»¤ë„ì•„ì¹´ë°ë¯¸ë¶€íŠ¸ìº í”„ #ì»¤ë„ì•„ì¹´ë°ë¯¸AIë¶€íŠ¸ìº í”„ #Kaggle`


