---
layout: single
title: "Kaggle training failure and lessons from the Community🧠"
---

# Kernel Academy AI Bootcamp (October 7, 2025)

---

## 🧨 Training Failed and Resource Limitations

- After a full 10-hour training session, the model failed to save due to an unexpected error
- GPU usage limit reached, so further training wasn't possible
- This reminded me how crucial **planning experiments** and managing resources properly is

---

## 🧭 What I Learned from Reading the Community Posts

Since I couldn't continue experimenting, I spent time reading through the **Kaggle competition community posts** in reverse order — about **20% of the total posts** so far.  
To my surprise, I found several key insights and **realized I had misunderstood important rules**.

### ❗ Misinterpreting the Competition Rules

> I misunderstood the "GPU usage within 10 hours" rule as a **training time constraint**, but it **actually applies only to inference time**.

This mistake led me to unnecessarily restrict my training time and, in hindsight, **hurt model performance**.

![community](/assets/images/kaggle-day5.jpg)

---

## 📦 Learning from Experts: Efficient and Fast Training is Possible

While browsing community threads, I came across a post by a **Kaggle Master** who trained a **Qwen3-4b** model.

- Completed training in **under 2 hours**
- Inference was also very fast
- In contrast, my setup:  
  - Training: **10+ hours (1 epoch, 1 fold)**  
  - Inference: **Over 5 hours**

> Even though we used similar models, the performance and efficiency were completely different.  
> It’s clear that **there’s a structural issue in my code or training strategy**.

---

## 🎯 Reflection and Next Steps

This was my **first Kaggle competition**, and I now realize how much I **missed out by not reading the rules and community threads early on**.  
If I had done so, I could have saved resources and possibly achieved better results.

For the remaining holiday, my plan is to:

- 📘 Read more community discussions  
- 🔍 Study code and strategies from experienced participants  
- ⚙️ Optimize inference time and refactor my code

I’m motivated to work even harder moving forward.

---

#### 🔖 Hashtags  
`#AI부트캠프 #커널아카데미 #커널아카데미부트캠프 #커널아카데미AI부트캠프 #Kaggle`
